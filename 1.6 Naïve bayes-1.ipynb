{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a083fe-5010-483e-94b7-a67d9eaa7dd5",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "### Q1. What is Bayes' theorem?\r\n",
    "**Answer:**\r\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics that describes the relationship between conditional probabilities. It provides a way to update the probability of a hypothesis based on new evidence. In other words, it allows us to revise our beliefs in light of new information.\r\n",
    "\r\n",
    "**Explanation:**\r\n",
    "The theorem shows how to calculate the probability of a hypothesis \\( H \\) given the evidence \\( E \\) by relating it to the probability of the evidence given the hypothesis, along with the prior probability of the hypothesis and the overall probability of the evidence.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Q2. What is the formula for Bayes' theorem?\r\n",
    "**Answer:**\r\n",
    "The formula for Bayes' theorem igivA B:\r\n",
    "\r",
    "(HB A .{PAE  /  HB ct P(H)}{P(E)}\r\n",
    "\\A\r\n",
    "\r",
    "BWhere:\r\n",
    "- \\( P(H | E) \\) is the posterior probability: the probability oA the hypothesis \\( H \\) after observBng the evideBce A( E \\).\r\n",
    "- \\( P(E | H) \\) is the likelihood: the probability of oBserving evidence \\A E \\) given that \\( A \\) is true.\r\n",
    "- \\( P(H) \\) is the prior probability: the initial probability of the hypothesis before seeingBthe evidence.\r\n",
    "- \\( P(E) \\) is the marginal likelihood: the total probability ofBobserving evidence \\( E \\) under all possible hypotheses.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Q3. How is Bayes' theorem used in practice?\r\n",
    "**Answer:**\r\n",
    "Bayes' theorem is widely used in various fields, including:\r\n",
    "\r\n",
    "1. **Spam Detection**: Email services use Naive Bayes classifiers to filter out spam by calculating the probability of an email being spam based on its content.\r\n",
    "2. **Medical Diagnosis**: Doctors can use Bayes' theorem to determine the likelihood of a disease given a patient's symptoms and prior probabilities of diseases.\r\n",
    "3. **Machine Learning**: Naive Bayes is a popular classification algorithm used for tasks such as text classification, sentiment analysis, and recommendation systems.\r\n",
    "\r\n",
    "**Explanation:**\r\n",
    "In practice, Bayes' theorem helps to make probabilistic predictions and decisions, allowing for the incorporation of new evidence and learning from data.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\r\n",
    "**Answer:**\r\n",
    "Bayes' theorem is built upon the concept of conditional probability. It expresses the relationship between the conditional probabilities of two events. Specifically, it allows us to compute the conditional probability of a hypothesis based on observed evidence and the conditional probability of observing that evidence given the hypothesis.\r\n",
    "\r\n",
    "**Explanation:**\r\n",
    "The theorem essentially shows how to flip the conditioning: it relates \\( P(H | E) \\) to \\( P(E | H) \\), demonstrating that knowing how likely evidence is given a hypothesis can inform us about how likely the hypothesis is given that evidence.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\r\n",
    "**Answer:**\r\n",
    "The choice of which type of Naive Bayes classifier to use depends on the nature of the features in the dataset:\r\n",
    "\r\n",
    "1. **Gaussian Naive Bayes**: Use this when your features are continuous and follow a normal distribution.\r\n",
    "2. **Multinomial Naive Bayes**: Suitable for discrete features, especially when working with text data where the features represent counts or frequencies (e.g., word counts).\r\n",
    "3. **Bernoulli Naive Bayes**: This classifier is effective when your features are binary (0s and 1s), such as the presence or absence of a feature.\r\n",
    "\r\n",
    "**Explanation:**\r\n",
    "Selecting the appropriate classifier is crucial for achieving good performance, as it aligns with the underlying assumptions aboute implementations for any of these concepts, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007bc07-e8c5-4851-86b4-475277ca48d2",
   "metadata": {},
   "source": [
    "Q6. Assignment:\r\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\r\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\r\n",
    "each feature value for each class:\r\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\r\n",
    "A 3 3 4 4 3 3 3\r\n",
    "B 2 2 1 2 2 2 3\r\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\r\n",
    "to belong to?\n",
    "\n",
    "\n",
    "\r\n",
    "1. **Calculate the total frequency of each class.**\r\n",
    "2. **Calculate the probability of each feature given each class.**\r\n",
    "3. **Combine these probabilities to find the class that maximizes the posterior probability for the new instance with features \\( X1 = 3 \\) and \\( X2 = 4 \\).**\r\n",
    "\r\n",
    "### Step 1: Total Frequency of Each Class\r\n",
    "From the table provided:\r\n",
    "\r\n",
    "- **Class A:**\r\n",
    "  - \\( X1 = 1 \\): 3\r\n",
    "  - \\( X1 = 2 \\): 3\r\n",
    "  - \\( X1 = 3 \\): 4\r\n",
    "  - \\( X1 = 4 \\): 3\r\n",
    "  - \\( X2 = 1 \\): 4\r\n",
    "  - \\( X2 = 2 \\): 3\r\n",
    "  - \\( X2 = 3 \\): 3\r\n",
    "  - \\( X2 = 4 \\): 3\r",
    "al fo Class A:**\r\n",
    "\\[\r\n",
    "\\text{Total}_A = 3 + 3 4 + 3 + 4 + 3 + 3 + 3 = 26\r\n",
    "\\]\r\n",
    "\r\n",
    "- **Class B:**\r\n",
    "  - \\( X1 = 1 \\): 2\r\n",
    "  - \\( X1 = 2 \\): 2\r\n",
    "  - \\( X1 = 3 \\): 1\r\n",
    "  - \\( X1 = 4 \\): 2\r\n",
    "  - \\( X2 = 1 \\): 2\r\n",
    "  - \\( X2 = 2 \\): 2\r\n",
    "  - \\( X2 = 3 \\): 2\r\n",
    "  - \\( X2 = 4 \\): 3\r\n",
    "\r\n",
    "**Total for Class B:**\r\n",
    "\\[\r\n",
    "\\text{Total}_B = 2 + 2 + 1 + 2 + 2 + 2 + 2 + 3 = 16\r\n",
    "\\]\r\n",
    "\r\n",
    "### Step 2: Calculate Probabilities\r\n",
    "Given equal prior probabilities for each class, we can assume:\r\n",
    "\\[\r\n",
    "P(A) = P(B) = \\frac{1}{2}\r\n",
    "\\]\r\n",
    "\r\n",
    "Now, let's calculate the probabilities of each feature given each class:\r\n",
    "\r\n",
    "**For Class A:**\r\n",
    "- \\( P(X1 = 3 | A) = \\frac{4}{26} \\)\r\n",
    "- \\( P(X2 = 4 | A) = \\frac{3}{26} \\)\r\n",
    "\r\n",
    "**For Class B:**\r\n",
    "- \\( P(X1 = 3 | B) = \\frac{1}{16} \\)\r\n",
    "- \\( P(X2 = 4 | B) = \\frac{3}{16} \\)\r\n",
    "\r\n",
    "### Step 3: Calculate Posterior Probabilities\r\n",
    "Using Bayes' theorem:\r\n",
    "\\[\r\n",
    "P(A | X1 = 3, X2 = 4) \\propto P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\cdot P(A)\r\n",
    "\\]\r\n",
    "\\[\r\n",
    "P(B | X1 = 3, X2 = 4) \\propto P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\cdot P(B)\r\n",
    "\\]\r\n",
    "\r\n",
    "#### Calculate for Class A:\r\n",
    "\\[\r\n",
    "P(A | X1 = 3, X2 = 4) \\propto \\frac{4}{26} \\cdot \\frac{3}{26} \\cdot \\frac{1}{2}\r\n",
    "\\]\r\n",
    "\\[\r\n",
    "= \\frac{12}{676} = \\frac{3}{169}\r\n",
    "\\]\r\n",
    "\r\n",
    "#### Calculate for Class B:\r\n",
    "\\[\r\n",
    "P(B | X1 = 3, X2 = 4) \\propto \\frac{1}{16} \\cdot \\frac{3}{16} \\cdot \\frac{1}{2}\r\n",
    "\\]\r\n",
    "\\[\r\n",
    "= \\frac{3}{512}\r\n",
    "\\]\r\n",
    "\r\n",
    "### Step 4: Compare Posterior Probabilities\r\n",
    "Now we need to compare \\( \\frac{3}{169} \\) and \\( \\frac{3}{512} \\).\r\n",
    "\r\n",
    "1. **Convert both fractions to a common denominator or decimal:**\r\n",
    "   - \\( \\frac{3}{169} \\approx 0.01775 \\)\r\n",
    "   - \\( \\frac{3}{512} \\approx 0.00586 \\)\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "Since \\( P(A | X1 = 3, X2 = 4) > P(B | X1 = 3, X2 = 4) \\):\r\n",
    "\r\n",
    "**Naive Bayes predicts that the new instance with features \\( X1 = 3 \\) and \\( X2 = 4 \\) belongs to Class A.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24387645-0ab6-4f83-b5f2-9d35222031fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
